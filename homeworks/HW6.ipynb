{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "\n",
    "## EMNIST Dataset\n",
    "\n",
    "[EMNIST dataset](https://www.nist.gov/itl/iad/image-group/emnist-dataset) is a set of hand-written characters and digits. Each of the data points is a grayscale image of size 28x28 pixels.  The structure of the dataset is the same as the infamous [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, but this dataset contains more samples and also contains characters. You can find more information on the dataset in the paper available at [https://arxiv.org/abs/1702.05373v1](https://arxiv.org/abs/1702.05373v1)\n",
    "\n",
    "You can find the dataset you need [at this link](https://www.dropbox.com/sh/vgap8ici7xs5w7f/AACE-9RrDpbGCc6bP72gHRfUa?dl=0).  Please download and use your local copy to do the homework.\n",
    "\n",
    "## Task 1\n",
    "\n",
    "Ingest the data (both the train and test sets) into this pyhthon notebook as a numpy array.\n",
    "\n",
    "\n",
    "## Task 2\n",
    "\n",
    "Write a convolutional artifial neural network model, train it and test it.\n",
    "\n",
    "\n",
    "## Notes\n",
    "\n",
    "1. You need to document each of your steps in both the ingestion phase and processing phase: explain the steps taken, the problems you encounter, how you solved them.\n",
    "\n",
    "2. DO NOT write python classes.  In other words, I do not want to see `__init__` or `__main__` in your code.  They are hard to follow (as they contain mutable state) and hard to port to future code you might write on a similar project.\n",
    "\n",
    "3. When you upload your solution to github, DO NOT include the datasets. They are large and I already have copies. I can test your models on the copy I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "İhtiyacımız olan numpy ,keras, pandas gibi kütüphaneleri yüklüyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "x_train, y_train = loadlocal_mnist(images_path = 'C:/Users/Halil/Desktop/EMNIST/emnist-balanced-train-images-idx3-ubyte',\n",
    "labels_path = 'C:/Users/Halil/Desktop/EMNIST/emnist-balanced-train-labels-idx1-ubyte')\n",
    "\n",
    "x_test, y_test = loadlocal_mnist(images_path = 'C:/Users/Halil/Desktop/EMNIST/emnist-balanced-test-images-idx3-ubyte', \n",
    "labels_path = 'C:/Users/Halil/Desktop/EMNIST/emnist-balanced-test-labels-idx1-ubyte')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train ve Test datalarını ayrı ayrı içeri aktarıp x_train, y_train ve x_test, y_test olarak atama işlemini gerçekleştiriyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([45, 36, 43, ..., 23, 31,  8], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_train datasını \"One hot encoding converts\" algoritması ile datayı 0 ve 1'lerden oluşan binary(ikili)'lere çevirttik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y _train 0 ve 1'lerden oluşan matrix haline geldi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(64, (4,4), input_shape=(28,28,1,), activation='tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras’ın Sequential modelini kullanıyoruz. Bu model sıralı katmanlardan oluşuyor. Activation kısmı ise katmanların output kısımlarına takıyoruz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Conv1D, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(47, activation='sigmoid'))\n",
    "model.add(Dropout(0.05))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Katmanları sırasıyla oluşturuyoruz. Bunun için .add() fonksiyonunu kullanıyoruz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MaxPooling2D işlemi sayesinde verimizi 2,2 boyutunda kümeler alıp bu küme içerisindeki en büyük değerleri kullanarak yeni bir matrix oluşturur. MaxPooling fonksiyou overfit durumunu engeller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten metodu ile çok boyutlu olan verimizi tek boyutlu hale getiriyoruz.\n",
    "Dense ile yapay sinir ağı katmanı oluşturup, 47 adet nöron bandırıyor.\n",
    "Dropout ile train matrisinden rastgele veri silip etkisiz hale getirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "112800/112800 [==============================] - 308s 3ms/step - loss: 1.6672 - acc: 0.6986\n",
      "Epoch 2/10\n",
      "112800/112800 [==============================] - 298s 3ms/step - loss: 1.2740 - acc: 0.7986\n",
      "Epoch 3/10\n",
      "112800/112800 [==============================] - 276s 2ms/step - loss: 1.2228 - acc: 0.8163\n",
      "Epoch 4/10\n",
      "112800/112800 [==============================] - 222s 2ms/step - loss: 1.1695 - acc: 0.8269\n",
      "Epoch 5/10\n",
      "112800/112800 [==============================] - 221s 2ms/step - loss: 1.1414 - acc: 0.8355\n",
      "Epoch 6/10\n",
      "112800/112800 [==============================] - 220s 2ms/step - loss: 1.0902 - acc: 0.8429\n",
      "Epoch 7/10\n",
      "112800/112800 [==============================] - 220s 2ms/step - loss: 1.1031 - acc: 0.8473\n",
      "Epoch 8/10\n",
      "112800/112800 [==============================] - 289s 3ms/step - loss: 1.0857 - acc: 0.8514\n",
      "Epoch 9/10\n",
      "112800/112800 [==============================] - 305s 3ms/step - loss: 1.0654 - acc: 0.8555\n",
      "Epoch 10/10\n",
      "112800/112800 [==============================] - 320s 3ms/step - loss: 1.0548 - acc: 0.8595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256c2b42780>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='RMSProp')\n",
    "model.fit(x_train.reshape(len(x_train),28,28,1), y_train, batch_size=512, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Değerlerin hesaplanmasında model.compile metoduna verdiğimiz parametreler kullanılır.\n",
    "Çıktı olarak dönen Loss train datasındaki hata oranı ve başa başarımdır\n",
    "Apochta gördüğümüz üzere test datamız üzerinde %85 lik başarı ile tahmin yürütebiliyoruz."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
